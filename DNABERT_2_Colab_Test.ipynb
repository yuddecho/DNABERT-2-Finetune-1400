{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ace63f4",
   "metadata": {
    "id": "9ace63f4"
   },
   "source": [
    "Change Colab runtime hardware gas pedal to **T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09bd2b43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09bd2b43",
    "outputId": "e8bc6b03-173b-44fe-ea89-e6885fc750d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m41.0/43.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m890.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c15b459",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c15b459",
    "outputId": "34964e6b-36a7-4aae-8024-1248fe131286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: triton 2.0.0\n",
      "Uninstalling triton-2.0.0:\n",
      "  Successfully uninstalled triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall triton --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "IgqvZub6nzZ6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IgqvZub6nzZ6",
    "outputId": "fe3595d8-b5c1-473b-db15-b2a7181f3035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting triton==2.0.0.dev20221202\n",
      "  Downloading triton-2.0.0.dev20221202-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0.dev20221202) (3.27.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0.dev20221202) (3.14.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0.dev20221202) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (11.7.91)\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch (from triton==2.0.0.dev20221202)\n",
      "  Using cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0.dev20221202) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting torch (from triton==2.0.0.dev20221202)\n",
      "  Using cached torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "  Using cached torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting torch (from triton==2.0.0.dev20221202)\n",
      "  Using cached torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "  Using cached torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch->triton==2.0.0.dev20221202)\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Collecting torch (from triton==2.0.0.dev20221202)\n",
      "  Using cached torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m520.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->triton==2.0.0.dev20221202) (67.7.2)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->triton==2.0.0.dev20221202) (0.43.0)\n",
      "Installing collected packages: torch, triton\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 1.13.1 which is incompatible.\n",
      "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.13.1 which is incompatible.\n",
      "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.13.1 triton-2.0.0.dev20221202\n"
     ]
    }
   ],
   "source": [
    "!pip install triton==2.0.0.dev20221202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24d4e98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b24d4e98",
    "outputId": "b20c5943-f895-4ba6-b5b5-ec9cb464a762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "4.41.2\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "print(triton.__version__)\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e2d528a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e2d528a",
    "outputId": "d09cbf67-d271-425a-bc67-95cec240177d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DNABERT-2-Finetune-1400'...\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
      "remote: Total 27 (delta 3), reused 24 (delta 3), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (27/27), 70.96 KiB | 14.19 MiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n",
      "Filtering content: 100% (2/2), 446.64 MiB | 14.38 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/yuddecho/DNABERT-2-Finetune-1400.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4242b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# merge, get model bin\n",
    "bin_file_dir = 'pytorch_model_files'\n",
    "if not os.path.exists(bin_file_dir):\n",
    "    raise\n",
    "    \n",
    "num_files = 10\n",
    "total_contents = bytes()\n",
    "\n",
    "for i in range(num_files):\n",
    "    file_path = f'/content/DNABERT-2-Finetune-1400/pytorch_model_{i}.bin'\n",
    "    \n",
    "    with open(file_path, 'rb') as infile:\n",
    "        total_contents += infile.read()\n",
    "      \n",
    "# write\n",
    "with open(f'/content/DNABERT-2-Finetune-1400/pytorch_model.bin', 'wb') as outfile:\n",
    "    outfile.write(total_contents)\n",
    "    \n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea30f04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bea30f04",
    "outputId": "dc281ce4-5bfe-4daa-e92f-ad2fd3d949aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_id = '/content/DNABERT-2-Finetune-1400'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n",
    "print(model)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac6ab88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dac6ab88",
    "outputId": "48e4b14e-3551-4ad7-b638-10debc9719c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "dna = \"ACGTAGCATCGGATCTATCTATCGACACTTGGTTATCGATCTACGAGCATCTCGTTAGC\"\n",
    "inputs = tokenizer(dna, return_tensors = 'pt')[\"input_ids\"]\n",
    "\n",
    "inputs = inputs.cuda()\n",
    "\n",
    "hidden_states = model(inputs)[0] # [1, sequence_length, 768]\n",
    "\n",
    "# embedding with mean pooling\n",
    "embedding_mean = torch.mean(hidden_states[0], dim=0)\n",
    "print(embedding_mean.shape) # expect to be 768\n",
    "\n",
    "# embedding with max pooling\n",
    "embedding_max = torch.max(hidden_states[0], dim=0)[0]\n",
    "print(embedding_max.shape) # expect to be 768"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
